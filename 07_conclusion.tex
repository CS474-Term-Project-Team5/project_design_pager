 In summary, Team 5's pipeline consists of a data preprocessing and two separate clustering steps. In the data preprocessing, first, we parse the raw data provided into a format appropriate for our tasks, and after that, we extract keywords and named entities. If we skip preprocessing, we would have to feed the algorithms in the further clustering processes the entire pure text data, which may lead in hindering of the results due to negligible terms 'screening' the terms that are actually related to the issue.\\
 Now we know which keywords and named entities represent each article. We would still have to transform this into a format that a computer can understand. We could use Bag of Words method, or a pre-trained model of BERT to turn a word or a phrase into embedding. Since both methods have distinct advantages and disadvantages, we will select one according to the quality of result in the further experiment, or choose to mix the two methods to produce the embedding of each article.\\
 With these embeddings, we will cluster every article published in one year time period using methods stated above; k-means, hierarchical document clustering, or DBSCAN. We assume that each cluster implies one unique issue of that year. Then, we can list each issue in order of how many articles are included in each cluster. We could assign a title for each cluster (or issue) using the most frequently occurring N-gram in the cluster, and print them accordingly, which is what the first task of issue trend analysis requires to do.\\
 In the second clustering, we will produce events from each issue cluster. This could be done by LDA which is a topic modelling algorithm. This will produce a topic distribution for each article. We assume here that each topic corresponds to a single unique event. After that, by using the method introduced in the section 6.1 events will be ordered in chronological order, and could be used to complete the On-issue event tracking task. For related-issue event tracking, we could calculate the distance between embeddings of each issue cluster to define 'how related' two issues are to each other. Then, we print out events in the nearest issue clusters in order of frequency of occurrence. For both subtasks, detailed information per event could be represented by named entities produced in the preprocessing steps.