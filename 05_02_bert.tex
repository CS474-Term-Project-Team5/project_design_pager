The another method is using transformer-based models such as BERT. Pre-trained models are especially helpful as they are supposed to contain more accurate representations of words and sentences. From the paper, \textbf{\textit{Angelov, D. (2020). Top2Vec: Distributed Representations of Topics}}, we thought the BERT based topic modeling can also be used for our term project. Based on this paper, after clustering with HDBSCAN and UMAP, topics are created by class-based TF-IDF.